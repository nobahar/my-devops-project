
==> Audit <==
|----------------|---------------------------|----------|---------|---------|-----------------------|-----------------------|
|    Command     |           Args            | Profile  |  User   | Version |      Start Time       |       End Time        |
|----------------|---------------------------|----------|---------|---------|-----------------------|-----------------------|
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 06 Feb 25 19:20 +0330 |                       |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 06 Feb 25 20:19 +0330 |                       |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 06 Feb 25 20:27 +0330 | 06 Feb 25 20:27 +0330 |
| service        | my-app-service --url      | minikube | hossein | v1.35.0 | 07 Feb 25 12:22 +0330 |                       |
| service        | my-app-service --url      | minikube | hossein | v1.35.0 | 07 Feb 25 12:24 +0330 |                       |
| service        | list                      | minikube | hossein | v1.35.0 | 07 Feb 25 12:24 +0330 | 07 Feb 25 12:24 +0330 |
| service        | my-app-service --url      | minikube | hossein | v1.35.0 | 07 Feb 25 12:27 +0330 |                       |
| service        | my-app-service -n default | minikube | hossein | v1.35.0 | 07 Feb 25 12:29 +0330 |                       |
| service        | list                      | minikube | hossein | v1.35.0 | 07 Feb 25 12:29 +0330 | 07 Feb 25 12:29 +0330 |
| tunnel         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 13:23 +0330 | 07 Feb 25 13:25 +0330 |
| service        | my-app-service --url      | minikube | hossein | v1.35.0 | 07 Feb 25 13:25 +0330 |                       |
| tunnel         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 13:25 +0330 | 07 Feb 25 13:26 +0330 |
| service        | my-app-service --url      | minikube | hossein | v1.35.0 | 07 Feb 25 13:26 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 13:26 +0330 | 07 Feb 25 13:26 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 13:27 +0330 |                       |
| start          |                           | minikube | hossein | v1.35.0 | 07 Feb 25 13:28 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 13:33 +0330 | 07 Feb 25 13:33 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 13:34 +0330 | 07 Feb 25 13:50 +0330 |
| addons         | list                      | minikube | hossein | v1.35.0 | 07 Feb 25 19:35 +0330 | 07 Feb 25 19:35 +0330 |
| addons         | enable ingress            | minikube | hossein | v1.35.0 | 07 Feb 25 19:35 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 19:47 +0330 | 07 Feb 25 19:47 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 19:49 +0330 |                       |
| update-context |                           | minikube | hossein | v1.35.0 | 07 Feb 25 19:53 +0330 | 07 Feb 25 19:53 +0330 |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 19:54 +0330 | 07 Feb 25 19:54 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 19:55 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 19:58 +0330 | 07 Feb 25 19:59 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 20:09 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 20:09 +0330 | 07 Feb 25 20:09 +0330 |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 20:12 +0330 | 07 Feb 25 20:12 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 20:12 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 07 Feb 25 20:15 +0330 | 07 Feb 25 20:15 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 07 Feb 25 20:16 +0330 |                       |
| update-context |                           | minikube | hossein | v1.35.0 | 08 Feb 25 10:09 +0330 | 08 Feb 25 10:09 +0330 |
| delete         |                           | minikube | hossein | v1.35.0 | 08 Feb 25 10:09 +0330 | 08 Feb 25 10:10 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 08 Feb 25 10:10 +0330 |                       |
| delete         |                           | minikube | hossein | v1.35.0 | 08 Feb 25 10:22 +0330 | 08 Feb 25 10:22 +0330 |
| start          | --driver=docker           | minikube | hossein | v1.35.0 | 08 Feb 25 10:22 +0330 | 08 Feb 25 10:26 +0330 |
| addons         | list                      | minikube | hossein | v1.35.0 | 08 Feb 25 11:31 +0330 | 08 Feb 25 11:31 +0330 |
| addons         | enable ingress            | minikube | hossein | v1.35.0 | 08 Feb 25 11:32 +0330 |                       |
|----------------|---------------------------|----------|---------|---------|-----------------------|-----------------------|


==> Last Start <==
Log file created at: 2025/02/08 10:22:57
Running on machine: hp
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0208 10:22:57.057652  539151 out.go:345] Setting OutFile to fd 1 ...
I0208 10:22:57.057834  539151 out.go:397] isatty.IsTerminal(1) = true
I0208 10:22:57.057845  539151 out.go:358] Setting ErrFile to fd 2...
I0208 10:22:57.057861  539151 out.go:397] isatty.IsTerminal(2) = true
I0208 10:22:57.058195  539151 root.go:338] Updating PATH: /home/hossein/.minikube/bin
I0208 10:22:57.058987  539151 out.go:352] Setting JSON to false
I0208 10:22:57.061638  539151 start.go:129] hostinfo: {"hostname":"hp","uptime":152118,"bootTime":1738845459,"procs":291,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.8.0-41-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d2d08c41-84c8-49c0-a381-e12164ac3754"}
I0208 10:22:57.061824  539151 start.go:139] virtualization:  
I0208 10:22:57.067010  539151 out.go:177] üòÑ  minikube v1.35.0 on Ubuntu 24.04
I0208 10:22:57.077789  539151 notify.go:220] Checking for updates...
I0208 10:22:57.078285  539151 driver.go:394] Setting default libvirt URI to qemu:///system
I0208 10:22:57.118492  539151 docker.go:123] docker version: linux-27.5.1:Docker Engine - Community
I0208 10:22:57.118623  539151 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0208 10:22:57.209670  539151 info.go:266] docker info: {ID:31eadb86-13e2-4478-8bc3-7d40a15d8ae0 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:43 SystemTime:2025-02-08 10:22:57.193839017 +0330 +0330 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-41-generic OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3979776000 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hp Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0208 10:22:57.209845  539151 docker.go:318] overlay module found
I0208 10:22:57.226556  539151 out.go:177] ‚ú®  Using the docker driver based on user configuration
I0208 10:22:57.230846  539151 start.go:297] selected driver: docker
I0208 10:22:57.230897  539151 start.go:901] validating driver "docker" against <nil>
I0208 10:22:57.230965  539151 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0208 10:22:57.231603  539151 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0208 10:22:57.324980  539151 info.go:266] docker info: {ID:31eadb86-13e2-4478-8bc3-7d40a15d8ae0 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:43 SystemTime:2025-02-08 10:22:57.310120403 +0330 +0330 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-41-generic OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:3979776000 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:hp Labels:[] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4]] Warnings:<nil>}}
I0208 10:22:57.325254  539151 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0208 10:22:57.326103  539151 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=3795MB, container=3795MB
I0208 10:22:57.326591  539151 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0208 10:22:57.330733  539151 out.go:177] üìå  Using Docker driver with root privileges
I0208 10:22:57.334518  539151 cni.go:84] Creating CNI manager for ""
I0208 10:22:57.334720  539151 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0208 10:22:57.334748  539151 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0208 10:22:57.334943  539151 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hossein:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0208 10:22:57.339887  539151 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0208 10:22:57.350060  539151 cache.go:121] Beginning downloading kic base image for docker with docker
I0208 10:22:57.354708  539151 out.go:177] üöú  Pulling base image v0.0.46 ...
I0208 10:22:57.363984  539151 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0208 10:22:57.364115  539151 preload.go:146] Found local preload: /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0208 10:22:57.364142  539151 cache.go:56] Caching tarball of preloaded images
I0208 10:22:57.364152  539151 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0208 10:22:57.364691  539151 preload.go:172] Found /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0208 10:22:57.364742  539151 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0208 10:22:57.366314  539151 profile.go:143] Saving config to /home/hossein/.minikube/profiles/minikube/config.json ...
I0208 10:22:57.366420  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/config.json: {Name:mk0597ff9b6269c1e9b377c584ba1c23f75a8751 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:22:57.403032  539151 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0208 10:22:57.403233  539151 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0208 10:22:57.403269  539151 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0208 10:22:57.403274  539151 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0208 10:22:57.403288  539151 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0208 10:22:57.403295  539151 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0208 10:22:57.403841  539151 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279, will try fallback image if available: tarball: unexpected EOF
I0208 10:22:57.403855  539151 image.go:81] Checking for docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0208 10:22:57.437557  539151 image.go:100] Found docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0208 10:22:57.437572  539151 cache.go:145] docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
W0208 10:22:57.437615  539151 out.go:270] ‚ùó  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.46, but successfully downloaded docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a fallback image
I0208 10:22:57.437653  539151 cache.go:227] Successfully downloaded all kic artifacts
I0208 10:22:57.437712  539151 start.go:360] acquireMachinesLock for minikube: {Name:mkc74335b8739ab3c52c0668363e37326df07a5b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0208 10:22:57.437820  539151 start.go:364] duration metric: took 78.453¬µs to acquireMachinesLock for "minikube"
I0208 10:22:57.437852  539151 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hossein:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0208 10:22:57.438023  539151 start.go:125] createHost starting for "" (driver="docker")
I0208 10:22:57.458342  539151 out.go:235] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0208 10:22:57.459562  539151 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0208 10:22:57.459696  539151 client.go:168] LocalClient.Create starting
I0208 10:22:57.459976  539151 main.go:141] libmachine: Reading certificate data from /home/hossein/.minikube/certs/ca.pem
I0208 10:22:57.460112  539151 main.go:141] libmachine: Decoding PEM data...
I0208 10:22:57.460150  539151 main.go:141] libmachine: Parsing certificate...
I0208 10:22:57.460391  539151 main.go:141] libmachine: Reading certificate data from /home/hossein/.minikube/certs/cert.pem
I0208 10:22:57.460767  539151 main.go:141] libmachine: Decoding PEM data...
I0208 10:22:57.460817  539151 main.go:141] libmachine: Parsing certificate...
I0208 10:22:57.461979  539151 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0208 10:22:57.495825  539151 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0208 10:22:57.495901  539151 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0208 10:22:57.495921  539151 cli_runner.go:164] Run: docker network inspect minikube
W0208 10:22:57.524554  539151 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0208 10:22:57.524580  539151 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0208 10:22:57.524618  539151 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0208 10:22:57.524741  539151 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0208 10:22:57.554346  539151 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0016903c0}
I0208 10:22:57.554382  539151 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0208 10:22:57.554455  539151 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0208 10:22:57.752401  539151 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0208 10:22:57.752431  539151 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0208 10:22:57.752545  539151 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0208 10:22:57.782606  539151 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0208 10:22:57.819105  539151 oci.go:103] Successfully created a docker volume minikube
I0208 10:22:57.819225  539151 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0208 10:22:59.298387  539151 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (1.479046145s)
I0208 10:22:59.298410  539151 oci.go:107] Successfully prepared a docker volume minikube
I0208 10:22:59.298449  539151 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0208 10:22:59.298470  539151 kic.go:194] Starting extracting preloaded images to volume ...
I0208 10:22:59.298542  539151 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
W0208 10:23:00.083841  539151 cli_runner.go:211] docker run --rm --entrypoint /usr/bin/tar -v /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir returned with exit code 2
I0208 10:23:00.083900  539151 kic.go:201] Unable to extract preloaded tarball to volume: docker run --rm --entrypoint /usr/bin/tar -v /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: exit status 2
stdout:

stderr:
tar (child): /preloaded.tar: Cannot read: Is a directory
tar (child): At beginning of tape, quitting now
tar (child): Error is not recoverable: exiting now
/usr/bin/tar: Child returned status 2
/usr/bin/tar: Error is not recoverable: exiting now
W0208 10:23:00.084035  539151 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0208 10:23:00.084090  539151 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0208 10:23:00.084140  539151 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0208 10:23:00.173908  539151 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0208 10:23:00.912731  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0208 10:23:00.970123  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:23:01.020626  539151 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0208 10:23:01.158912  539151 oci.go:144] the created container "minikube" has a running status.
I0208 10:23:01.158944  539151 kic.go:225] Creating ssh key for kic: /home/hossein/.minikube/machines/minikube/id_rsa...
I0208 10:23:01.450846  539151 kic_runner.go:191] docker (temp): /home/hossein/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0208 10:23:01.489264  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:23:01.522484  539151 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0208 10:23:01.522502  539151 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0208 10:23:01.601950  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:23:01.635392  539151 machine.go:93] provisionDockerMachine start ...
I0208 10:23:01.635536  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:01.674207  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:01.674555  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:01.674571  539151 main.go:141] libmachine: About to run SSH command:
hostname
I0208 10:23:01.675637  539151 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:45504->127.0.0.1:32813: read: connection reset by peer
I0208 10:23:04.856213  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0208 10:23:04.856246  539151 ubuntu.go:169] provisioning hostname "minikube"
I0208 10:23:04.856406  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:04.888237  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:04.888538  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:04.888552  539151 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0208 10:23:05.084515  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0208 10:23:05.084646  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:05.117398  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:05.117717  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:05.117739  539151 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0208 10:23:05.289644  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0208 10:23:05.289672  539151 ubuntu.go:175] set auth options {CertDir:/home/hossein/.minikube CaCertPath:/home/hossein/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hossein/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hossein/.minikube/machines/server.pem ServerKeyPath:/home/hossein/.minikube/machines/server-key.pem ClientKeyPath:/home/hossein/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hossein/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hossein/.minikube}
I0208 10:23:05.289698  539151 ubuntu.go:177] setting up certificates
I0208 10:23:05.289715  539151 provision.go:84] configureAuth start
I0208 10:23:05.289818  539151 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0208 10:23:05.323301  539151 provision.go:143] copyHostCerts
I0208 10:23:05.323384  539151 exec_runner.go:144] found /home/hossein/.minikube/ca.pem, removing ...
I0208 10:23:05.323400  539151 exec_runner.go:203] rm: /home/hossein/.minikube/ca.pem
I0208 10:23:05.323514  539151 exec_runner.go:151] cp: /home/hossein/.minikube/certs/ca.pem --> /home/hossein/.minikube/ca.pem (1078 bytes)
I0208 10:23:05.323892  539151 exec_runner.go:144] found /home/hossein/.minikube/cert.pem, removing ...
I0208 10:23:05.323905  539151 exec_runner.go:203] rm: /home/hossein/.minikube/cert.pem
I0208 10:23:05.323998  539151 exec_runner.go:151] cp: /home/hossein/.minikube/certs/cert.pem --> /home/hossein/.minikube/cert.pem (1123 bytes)
I0208 10:23:05.324284  539151 exec_runner.go:144] found /home/hossein/.minikube/key.pem, removing ...
I0208 10:23:05.324296  539151 exec_runner.go:203] rm: /home/hossein/.minikube/key.pem
I0208 10:23:05.324386  539151 exec_runner.go:151] cp: /home/hossein/.minikube/certs/key.pem --> /home/hossein/.minikube/key.pem (1679 bytes)
I0208 10:23:05.327479  539151 provision.go:117] generating server cert: /home/hossein/.minikube/machines/server.pem ca-key=/home/hossein/.minikube/certs/ca.pem private-key=/home/hossein/.minikube/certs/ca-key.pem org=hossein.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0208 10:23:05.909519  539151 provision.go:177] copyRemoteCerts
I0208 10:23:05.909588  539151 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0208 10:23:05.909639  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:05.941934  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:23:06.069108  539151 ssh_runner.go:362] scp /home/hossein/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0208 10:23:06.123859  539151 ssh_runner.go:362] scp /home/hossein/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0208 10:23:06.175346  539151 ssh_runner.go:362] scp /home/hossein/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0208 10:23:06.226746  539151 provision.go:87] duration metric: took 936.98189ms to configureAuth
I0208 10:23:06.226783  539151 ubuntu.go:193] setting minikube options for container-runtime
I0208 10:23:06.227062  539151 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0208 10:23:06.227137  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:06.264709  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:06.264989  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:06.265001  539151 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0208 10:23:06.434701  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0208 10:23:06.434724  539151 ubuntu.go:71] root file system type: overlay
I0208 10:23:06.434918  539151 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0208 10:23:06.434995  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:06.469520  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:06.469827  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:06.469969  539151 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0208 10:23:06.672835  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0208 10:23:06.672924  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:06.705469  539151 main.go:141] libmachine: Using SSH client type: native
I0208 10:23:06.705762  539151 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32813 <nil> <nil>}
I0208 10:23:06.705788  539151 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0208 10:23:08.533975  539151 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-02-08 06:53:06.669448305 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0208 10:23:08.534021  539151 machine.go:96] duration metric: took 6.89860811s to provisionDockerMachine
I0208 10:23:08.534044  539151 client.go:171] duration metric: took 11.074334238s to LocalClient.Create
I0208 10:23:08.534081  539151 start.go:167] duration metric: took 11.074537017s to libmachine.API.Create "minikube"
I0208 10:23:08.534094  539151 start.go:293] postStartSetup for "minikube" (driver="docker")
I0208 10:23:08.534120  539151 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0208 10:23:08.534295  539151 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0208 10:23:08.534378  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:08.567457  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:23:08.689812  539151 ssh_runner.go:195] Run: cat /etc/os-release
I0208 10:23:08.696653  539151 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0208 10:23:08.696697  539151 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0208 10:23:08.696712  539151 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0208 10:23:08.696722  539151 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0208 10:23:08.696736  539151 filesync.go:126] Scanning /home/hossein/.minikube/addons for local assets ...
I0208 10:23:08.699681  539151 filesync.go:126] Scanning /home/hossein/.minikube/files for local assets ...
I0208 10:23:08.699907  539151 start.go:296] duration metric: took 165.802755ms for postStartSetup
I0208 10:23:08.700484  539151 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0208 10:23:08.731627  539151 profile.go:143] Saving config to /home/hossein/.minikube/profiles/minikube/config.json ...
I0208 10:23:08.732078  539151 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0208 10:23:08.732131  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:08.765754  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:23:08.880146  539151 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0208 10:23:08.888127  539151 start.go:128] duration metric: took 11.450087741s to createHost
I0208 10:23:08.888149  539151 start.go:83] releasing machines lock for "minikube", held for 11.450312238s
I0208 10:23:08.888253  539151 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0208 10:23:08.918005  539151 ssh_runner.go:195] Run: cat /version.json
I0208 10:23:08.918082  539151 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0208 10:23:08.918119  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:08.918202  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:23:08.951735  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:23:08.953091  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:23:10.216429  539151 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.298306794s)
I0208 10:23:10.216550  539151 ssh_runner.go:235] Completed: cat /version.json: (1.298513437s)
I0208 10:23:10.216774  539151 ssh_runner.go:195] Run: systemctl --version
I0208 10:23:10.224943  539151 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0208 10:23:10.233195  539151 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0208 10:23:10.289445  539151 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0208 10:23:10.289543  539151 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0208 10:23:10.352931  539151 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0208 10:23:10.352956  539151 start.go:495] detecting cgroup driver to use...
I0208 10:23:10.353005  539151 detect.go:190] detected "systemd" cgroup driver on host os
I0208 10:23:10.353136  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0208 10:23:10.389594  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0208 10:23:10.409684  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0208 10:23:10.430285  539151 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0208 10:23:10.430375  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0208 10:23:10.450309  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0208 10:23:10.471818  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0208 10:23:10.493357  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0208 10:23:10.512916  539151 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0208 10:23:10.531630  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0208 10:23:10.551299  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0208 10:23:10.572797  539151 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0208 10:23:10.594339  539151 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0208 10:23:10.612109  539151 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0208 10:23:10.629555  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:23:10.790352  539151 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0208 10:23:10.945078  539151 start.go:495] detecting cgroup driver to use...
I0208 10:23:10.945130  539151 detect.go:190] detected "systemd" cgroup driver on host os
I0208 10:23:10.945228  539151 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0208 10:23:10.984212  539151 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0208 10:23:10.984312  539151 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0208 10:23:11.013397  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0208 10:23:11.055397  539151 ssh_runner.go:195] Run: which cri-dockerd
I0208 10:23:11.064146  539151 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0208 10:23:11.088072  539151 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0208 10:23:11.127602  539151 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0208 10:23:11.308267  539151 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0208 10:23:11.496848  539151 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0208 10:23:11.497049  539151 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0208 10:23:11.537209  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:23:11.711404  539151 ssh_runner.go:195] Run: sudo systemctl restart docker
I0208 10:23:12.748246  539151 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.036799357s)
I0208 10:23:12.748343  539151 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0208 10:23:12.773012  539151 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0208 10:23:12.799321  539151 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0208 10:23:12.960962  539151 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0208 10:23:13.114359  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:23:13.277288  539151 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0208 10:23:13.313982  539151 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0208 10:23:13.338837  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:23:13.506632  539151 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0208 10:23:13.644305  539151 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0208 10:23:13.644381  539151 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0208 10:23:13.652567  539151 start.go:563] Will wait 60s for crictl version
I0208 10:23:13.652656  539151 ssh_runner.go:195] Run: which crictl
I0208 10:23:13.659455  539151 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0208 10:23:13.727524  539151 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0208 10:23:13.727620  539151 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0208 10:23:13.769625  539151 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0208 10:23:13.832268  539151 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0208 10:23:13.832436  539151 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0208 10:23:13.863602  539151 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0208 10:23:13.870395  539151 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0208 10:23:13.895868  539151 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hossein:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0208 10:23:13.896045  539151 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0208 10:23:13.896128  539151 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0208 10:23:13.929098  539151 docker.go:689] Got preloaded images: 
I0208 10:23:13.929114  539151 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0208 10:23:13.929209  539151 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0208 10:23:13.946551  539151 ssh_runner.go:195] Run: which lz4
I0208 10:23:13.953201  539151 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0208 10:23:13.959577  539151 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0208 10:23:13.959609  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (4096 bytes)
I0208 10:23:14.002147  539151 kubeadm.go:909] preload failed, will try to load cached images: copying file: sudo mkdir -p / && sudo scp -t / && sudo touch -d "2025-02-07 20:25:00.070664108 +0330" /preloaded.tar.lz4: Process exited with status 1
output:   scp: Broken pipe
I0208 10:23:14.002293  539151 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0208 10:23:14.035769  539151 docker.go:689] Got preloaded images: 
I0208 10:23:14.035793  539151 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0208 10:23:14.035806  539151 cache_images.go:88] LoadCachedImages start: [registry.k8s.io/kube-apiserver:v1.32.0 registry.k8s.io/kube-controller-manager:v1.32.0 registry.k8s.io/kube-scheduler:v1.32.0 registry.k8s.io/kube-proxy:v1.32.0 registry.k8s.io/pause:3.10 registry.k8s.io/etcd:3.5.16-0 registry.k8s.io/coredns/coredns:v1.11.3 gcr.io/k8s-minikube/storage-provisioner:v5]
I0208 10:23:14.037492  539151 image.go:135] retrieving image: gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:23:14.038592  539151 image.go:135] retrieving image: registry.k8s.io/kube-proxy:v1.32.0
I0208 10:23:14.039835  539151 image.go:178] daemon lookup for gcr.io/k8s-minikube/storage-provisioner:v5: Error response from daemon: No such image: gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:23:14.041270  539151 image.go:135] retrieving image: registry.k8s.io/kube-apiserver:v1.32.0
I0208 10:23:14.041713  539151 image.go:178] daemon lookup for registry.k8s.io/kube-proxy:v1.32.0: Error response from daemon: No such image: registry.k8s.io/kube-proxy:v1.32.0
I0208 10:23:14.041745  539151 image.go:135] retrieving image: registry.k8s.io/etcd:3.5.16-0
I0208 10:23:14.042922  539151 image.go:178] daemon lookup for registry.k8s.io/kube-apiserver:v1.32.0: Error response from daemon: No such image: registry.k8s.io/kube-apiserver:v1.32.0
I0208 10:23:14.046293  539151 image.go:135] retrieving image: registry.k8s.io/coredns/coredns:v1.11.3
I0208 10:23:14.046980  539151 image.go:178] daemon lookup for registry.k8s.io/etcd:3.5.16-0: Error response from daemon: No such image: registry.k8s.io/etcd:3.5.16-0
I0208 10:23:14.048586  539151 image.go:135] retrieving image: registry.k8s.io/pause:3.10
I0208 10:23:14.050701  539151 image.go:178] daemon lookup for registry.k8s.io/coredns/coredns:v1.11.3: Error response from daemon: No such image: registry.k8s.io/coredns/coredns:v1.11.3
I0208 10:23:14.050712  539151 image.go:135] retrieving image: registry.k8s.io/kube-controller-manager:v1.32.0
I0208 10:23:14.052149  539151 image.go:135] retrieving image: registry.k8s.io/kube-scheduler:v1.32.0
I0208 10:23:14.052582  539151 image.go:178] daemon lookup for registry.k8s.io/pause:3.10: Error response from daemon: No such image: registry.k8s.io/pause:3.10
I0208 10:23:14.053315  539151 image.go:178] daemon lookup for registry.k8s.io/kube-controller-manager:v1.32.0: Error response from daemon: No such image: registry.k8s.io/kube-controller-manager:v1.32.0
I0208 10:23:14.054068  539151 image.go:178] daemon lookup for registry.k8s.io/kube-scheduler:v1.32.0: Error response from daemon: No such image: registry.k8s.io/kube-scheduler:v1.32.0
I0208 10:23:17.302551  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/coredns/coredns:v1.11.3
I0208 10:23:17.303593  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-apiserver:v1.32.0
I0208 10:23:17.306591  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-controller-manager:v1.32.0
I0208 10:23:17.307568  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-proxy:v1.32.0
I0208 10:23:17.315073  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/pause:3.10
I0208 10:23:17.315358  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/etcd:3.5.16-0
I0208 10:23:17.322421  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} registry.k8s.io/kube-scheduler:v1.32.0
I0208 10:23:17.418194  539151 cache_images.go:116] "registry.k8s.io/coredns/coredns:v1.11.3" needs transfer: "registry.k8s.io/coredns/coredns:v1.11.3" does not exist at hash "c69fa2e9cbf5f42dc48af631e956d3f95724c13f91596bc567591790e5e36db6" in container runtime
I0208 10:23:17.418258  539151 docker.go:337] Removing image: registry.k8s.io/coredns/coredns:v1.11.3
I0208 10:23:17.418337  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/coredns/coredns:v1.11.3
I0208 10:23:17.418551  539151 cache_images.go:116] "registry.k8s.io/kube-apiserver:v1.32.0" needs transfer: "registry.k8s.io/kube-apiserver:v1.32.0" does not exist at hash "c2e17b8d0f4a39ed32f1c1fd4eb408627c94111ae9a46c2034758e4ced4f79c4" in container runtime
I0208 10:23:17.418592  539151 docker.go:337] Removing image: registry.k8s.io/kube-apiserver:v1.32.0
I0208 10:23:17.418653  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-apiserver:v1.32.0
I0208 10:23:17.439927  539151 cache_images.go:116] "registry.k8s.io/kube-proxy:v1.32.0" needs transfer: "registry.k8s.io/kube-proxy:v1.32.0" does not exist at hash "040f9f8aac8cd21d78f05ebfa9621ffb84e3257300c3cb1f72b539a3c3a2cd08" in container runtime
I0208 10:23:17.439991  539151 docker.go:337] Removing image: registry.k8s.io/kube-proxy:v1.32.0
I0208 10:23:17.439994  539151 cache_images.go:116] "registry.k8s.io/kube-controller-manager:v1.32.0" needs transfer: "registry.k8s.io/kube-controller-manager:v1.32.0" does not exist at hash "8cab3d2a8bd0fe4127810f35afe0ffd42bfe75b2a4712a84da5595d4bde617d3" in container runtime
I0208 10:23:17.440045  539151 docker.go:337] Removing image: registry.k8s.io/kube-controller-manager:v1.32.0
I0208 10:23:17.440066  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-proxy:v1.32.0
I0208 10:23:17.440118  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-controller-manager:v1.32.0
I0208 10:23:17.440257  539151 cache_images.go:116] "registry.k8s.io/etcd:3.5.16-0" needs transfer: "registry.k8s.io/etcd:3.5.16-0" does not exist at hash "a9e7e6b294baf1695fccb862d956c5d3ad8510e1e4ca1535f35dc09f247abbfc" in container runtime
I0208 10:23:17.440294  539151 docker.go:337] Removing image: registry.k8s.io/etcd:3.5.16-0
I0208 10:23:17.440318  539151 cache_images.go:116] "registry.k8s.io/pause:3.10" needs transfer: "registry.k8s.io/pause:3.10" does not exist at hash "873ed75102791e5b0b8a7fcd41606c92fcec98d56d05ead4ac5131650004c136" in container runtime
I0208 10:23:17.440349  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/etcd:3.5.16-0
I0208 10:23:17.440357  539151 docker.go:337] Removing image: registry.k8s.io/pause:3.10
I0208 10:23:17.440430  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/pause:3.10
I0208 10:23:17.440492  539151 cache_images.go:116] "registry.k8s.io/kube-scheduler:v1.32.0" needs transfer: "registry.k8s.io/kube-scheduler:v1.32.0" does not exist at hash "a389e107f4ff1130c69849f0af08cbce9a1dfe3b7c39874012587d233807cfc5" in container runtime
I0208 10:23:17.440527  539151 docker.go:337] Removing image: registry.k8s.io/kube-scheduler:v1.32.0
I0208 10:23:17.440581  539151 ssh_runner.go:195] Run: docker rmi registry.k8s.io/kube-scheduler:v1.32.0
I0208 10:23:17.524206  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.3
I0208 10:23:17.524391  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/coredns_v1.11.3
I0208 10:23:17.524665  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.32.0
I0208 10:23:17.525321  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-apiserver_v1.32.0
I0208 10:23:17.564949  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.32.0
I0208 10:23:17.564977  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.32.0
I0208 10:23:17.565120  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-proxy_v1.32.0
I0208 10:23:17.565121  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-scheduler_v1.32.0
I0208 10:23:17.565122  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.32.0
I0208 10:23:17.565134  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.16-0
I0208 10:23:17.565259  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10
I0208 10:23:17.565291  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/etcd_3.5.16-0
I0208 10:23:17.565292  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/kube-controller-manager_v1.32.0
I0208 10:23:17.565380  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/pause_3.10
I0208 10:23:17.565399  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-apiserver_v1.32.0: stat -c "%s %y" /var/lib/minikube/images/kube-apiserver_v1.32.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-apiserver_v1.32.0': No such file or directory
I0208 10:23:17.565426  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.32.0 --> /var/lib/minikube/images/kube-apiserver_v1.32.0 (28680192 bytes)
I0208 10:23:17.565451  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/coredns_v1.11.3: stat -c "%s %y" /var/lib/minikube/images/coredns_v1.11.3: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/coredns_v1.11.3': No such file or directory
I0208 10:23:17.565476  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.3 --> /var/lib/minikube/images/coredns_v1.11.3 (18571264 bytes)
I0208 10:23:17.580079  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-scheduler_v1.32.0: stat -c "%s %y" /var/lib/minikube/images/kube-scheduler_v1.32.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-scheduler_v1.32.0': No such file or directory
I0208 10:23:17.580118  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.32.0 --> /var/lib/minikube/images/kube-scheduler_v1.32.0 (20666368 bytes)
I0208 10:23:17.582431  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-proxy_v1.32.0: stat -c "%s %y" /var/lib/minikube/images/kube-proxy_v1.32.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-proxy_v1.32.0': No such file or directory
I0208 10:23:17.582480  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.32.0 --> /var/lib/minikube/images/kube-proxy_v1.32.0 (30908928 bytes)
I0208 10:23:17.582563  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/etcd_3.5.16-0: stat -c "%s %y" /var/lib/minikube/images/etcd_3.5.16-0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/etcd_3.5.16-0': No such file or directory
I0208 10:23:17.582587  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.16-0 --> /var/lib/minikube/images/etcd_3.5.16-0 (57690112 bytes)
I0208 10:23:17.613212  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/pause_3.10: stat -c "%s %y" /var/lib/minikube/images/pause_3.10: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/pause_3.10': No such file or directory
I0208 10:23:17.613262  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 --> /var/lib/minikube/images/pause_3.10 (321024 bytes)
I0208 10:23:17.618042  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/kube-controller-manager_v1.32.0: stat -c "%s %y" /var/lib/minikube/images/kube-controller-manager_v1.32.0: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/kube-controller-manager_v1.32.0': No such file or directory
I0208 10:23:17.618081  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.32.0 --> /var/lib/minikube/images/kube-controller-manager_v1.32.0 (26265088 bytes)
I0208 10:23:17.898825  539151 docker.go:304] Loading image: /var/lib/minikube/images/pause_3.10
I0208 10:23:17.898858  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/pause_3.10 | docker load"
I0208 10:23:18.144435  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/pause_3.10 from cache
I0208 10:23:18.356303  539151 docker.go:304] Loading image: /var/lib/minikube/images/coredns_v1.11.3
I0208 10:23:18.356336  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/coredns_v1.11.3 | docker load"
I0208 10:23:18.581264  539151 ssh_runner.go:195] Run: docker image inspect --format {{.Id}} gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:23:21.642893  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/coredns_v1.11.3 | docker load": (3.286518839s)
I0208 10:23:21.642924  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/coredns/coredns_v1.11.3 from cache
I0208 10:23:21.642942  539151 ssh_runner.go:235] Completed: docker image inspect --format {{.Id}} gcr.io/k8s-minikube/storage-provisioner:v5: (3.061593027s)
I0208 10:23:21.642996  539151 docker.go:304] Loading image: /var/lib/minikube/images/kube-scheduler_v1.32.0
I0208 10:23:21.643007  539151 cache_images.go:116] "gcr.io/k8s-minikube/storage-provisioner:v5" needs transfer: "gcr.io/k8s-minikube/storage-provisioner:v5" does not exist at hash "6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562" in container runtime
I0208 10:23:21.643029  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-scheduler_v1.32.0 | docker load"
I0208 10:23:21.643061  539151 docker.go:337] Removing image: gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:23:21.643150  539151 ssh_runner.go:195] Run: docker rmi gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:23:24.080065  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-scheduler_v1.32.0 | docker load": (2.436999255s)
I0208 10:23:24.080098  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-scheduler_v1.32.0 from cache
I0208 10:23:24.080128  539151 docker.go:304] Loading image: /var/lib/minikube/images/kube-apiserver_v1.32.0
I0208 10:23:24.080146  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-apiserver_v1.32.0 | docker load"
I0208 10:23:24.080152  539151 ssh_runner.go:235] Completed: docker rmi gcr.io/k8s-minikube/storage-provisioner:v5: (2.436974689s)
I0208 10:23:24.080219  539151 cache_images.go:289] Loading image from: /home/hossein/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5
I0208 10:23:24.080375  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/images/storage-provisioner_v5
I0208 10:23:25.801506  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-apiserver_v1.32.0 | docker load": (1.721302349s)
I0208 10:23:25.801539  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-apiserver_v1.32.0 from cache
I0208 10:23:25.801571  539151 docker.go:304] Loading image: /var/lib/minikube/images/kube-controller-manager_v1.32.0
I0208 10:23:25.801569  539151 ssh_runner.go:235] Completed: stat -c "%s %y" /var/lib/minikube/images/storage-provisioner_v5: (1.721154785s)
I0208 10:23:25.801589  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-controller-manager_v1.32.0 | docker load"
I0208 10:23:25.801605  539151 ssh_runner.go:352] existence check for /var/lib/minikube/images/storage-provisioner_v5: stat -c "%s %y" /var/lib/minikube/images/storage-provisioner_v5: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/images/storage-provisioner_v5': No such file or directory
I0208 10:23:25.801638  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 --> /var/lib/minikube/images/storage-provisioner_v5 (9060352 bytes)
I0208 10:23:27.253943  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-controller-manager_v1.32.0 | docker load": (1.4523239s)
I0208 10:23:27.253972  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-controller-manager_v1.32.0 from cache
I0208 10:23:27.254003  539151 docker.go:304] Loading image: /var/lib/minikube/images/kube-proxy_v1.32.0
I0208 10:23:27.254015  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-proxy_v1.32.0 | docker load"
I0208 10:23:29.851722  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/kube-proxy_v1.32.0 | docker load": (2.59767845s)
I0208 10:23:29.851748  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/kube-proxy_v1.32.0 from cache
I0208 10:23:29.851781  539151 docker.go:304] Loading image: /var/lib/minikube/images/etcd_3.5.16-0
I0208 10:23:29.851791  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.16-0 | docker load"
I0208 10:23:33.879206  539151 ssh_runner.go:235] Completed: /bin/bash -c "sudo cat /var/lib/minikube/images/etcd_3.5.16-0 | docker load": (4.027344711s)
I0208 10:23:33.879252  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/registry.k8s.io/etcd_3.5.16-0 from cache
I0208 10:23:33.879297  539151 docker.go:304] Loading image: /var/lib/minikube/images/storage-provisioner_v5
I0208 10:23:33.879317  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo cat /var/lib/minikube/images/storage-provisioner_v5 | docker load"
I0208 10:23:34.654848  539151 cache_images.go:321] Transferred and loaded /home/hossein/.minikube/cache/images/amd64/gcr.io/k8s-minikube/storage-provisioner_v5 from cache
I0208 10:23:34.654936  539151 cache_images.go:123] Successfully loaded all cached images
I0208 10:23:34.654954  539151 cache_images.go:92] duration metric: took 20.61912312s to LoadCachedImages
I0208 10:23:34.654987  539151 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0208 10:23:34.655331  539151 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0208 10:23:34.655449  539151 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0208 10:23:34.753616  539151 cni.go:84] Creating CNI manager for ""
I0208 10:23:34.753648  539151 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0208 10:23:34.753665  539151 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0208 10:23:34.753704  539151 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0208 10:23:34.753921  539151 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0208 10:23:34.754014  539151 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0208 10:23:34.771886  539151 binaries.go:47] Didn't find k8s binaries: sudo ls /var/lib/minikube/binaries/v1.32.0: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/binaries/v1.32.0': No such file or directory

Initiating transfer...
I0208 10:23:34.771978  539151 ssh_runner.go:195] Run: sudo mkdir -p /var/lib/minikube/binaries/v1.32.0
I0208 10:23:34.790763  539151 binary.go:74] Not caching binary, using https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubelet?checksum=file:https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubelet.sha256
I0208 10:23:34.790787  539151 download.go:108] Downloading: https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubeadm?checksum=file:https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubeadm.sha256 -> /home/hossein/.minikube/cache/linux/amd64/v1.32.0/kubeadm
I0208 10:23:34.790786  539151 download.go:108] Downloading: https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubectl?checksum=file:https://dl.k8s.io/release/v1.32.0/bin/linux/amd64/kubectl.sha256 -> /home/hossein/.minikube/cache/linux/amd64/v1.32.0/kubectl
I0208 10:23:34.790848  539151 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0208 10:23:34.819704  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubelet
I0208 10:23:34.827461  539151 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.32.0/kubelet: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubelet: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.32.0/kubelet': No such file or directory
I0208 10:23:34.827520  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/linux/amd64/v1.32.0/kubelet --> /var/lib/minikube/binaries/v1.32.0/kubelet (77398276 bytes)
I0208 10:25:38.422965  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubeadm
I0208 10:25:38.432656  539151 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.32.0/kubeadm: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubeadm: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.32.0/kubeadm': No such file or directory
I0208 10:25:38.432702  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/linux/amd64/v1.32.0/kubeadm --> /var/lib/minikube/binaries/v1.32.0/kubeadm (70942872 bytes)
I0208 10:25:49.681332  539151 ssh_runner.go:195] Run: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubectl
I0208 10:25:49.688354  539151 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.32.0/kubectl: stat -c "%s %y" /var/lib/minikube/binaries/v1.32.0/kubectl: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.32.0/kubectl': No such file or directory
I0208 10:25:49.688383  539151 ssh_runner.go:362] scp /home/hossein/.minikube/cache/linux/amd64/v1.32.0/kubectl --> /var/lib/minikube/binaries/v1.32.0/kubectl (57323672 bytes)
I0208 10:25:50.063471  539151 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0208 10:25:50.085988  539151 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0208 10:25:50.123006  539151 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0208 10:25:50.161323  539151 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0208 10:25:50.199625  539151 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0208 10:25:50.210255  539151 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0208 10:25:50.233400  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:25:50.396193  539151 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0208 10:25:50.432194  539151 certs.go:68] Setting up /home/hossein/.minikube/profiles/minikube for IP: 192.168.49.2
I0208 10:25:50.432214  539151 certs.go:194] generating shared ca certs ...
I0208 10:25:50.432249  539151 certs.go:226] acquiring lock for ca certs: {Name:mk64def01d3307b1088c366ee23669b7f9dd57ac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:50.432915  539151 certs.go:235] skipping valid "minikubeCA" ca cert: /home/hossein/.minikube/ca.key
I0208 10:25:50.433270  539151 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/hossein/.minikube/proxy-client-ca.key
I0208 10:25:50.433291  539151 certs.go:256] generating profile certs ...
I0208 10:25:50.433422  539151 certs.go:363] generating signed profile cert for "minikube-user": /home/hossein/.minikube/profiles/minikube/client.key
I0208 10:25:50.433451  539151 crypto.go:68] Generating cert /home/hossein/.minikube/profiles/minikube/client.crt with IP's: []
I0208 10:25:51.105048  539151 crypto.go:156] Writing cert to /home/hossein/.minikube/profiles/minikube/client.crt ...
I0208 10:25:51.105070  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/client.crt: {Name:mk840306257aa577322ee3e531e5400411a9fdcc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:51.105301  539151 crypto.go:164] Writing key to /home/hossein/.minikube/profiles/minikube/client.key ...
I0208 10:25:51.105312  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/client.key: {Name:mk3afcdf93f0562dbba36b0f476961cf7a9a790c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:51.105450  539151 certs.go:363] generating signed profile cert for "minikube": /home/hossein/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0208 10:25:51.105474  539151 crypto.go:68] Generating cert /home/hossein/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0208 10:25:51.890476  539151 crypto.go:156] Writing cert to /home/hossein/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0208 10:25:51.890495  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk48007f2bf903eb890defd4075f7bfba8739f03 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:51.890731  539151 crypto.go:164] Writing key to /home/hossein/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0208 10:25:51.890743  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk203af53604f86fbf0874549df19ef1623f6531 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:51.890901  539151 certs.go:381] copying /home/hossein/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/hossein/.minikube/profiles/minikube/apiserver.crt
I0208 10:25:51.891038  539151 certs.go:385] copying /home/hossein/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/hossein/.minikube/profiles/minikube/apiserver.key
I0208 10:25:51.891180  539151 certs.go:363] generating signed profile cert for "aggregator": /home/hossein/.minikube/profiles/minikube/proxy-client.key
I0208 10:25:51.891208  539151 crypto.go:68] Generating cert /home/hossein/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0208 10:25:52.451814  539151 crypto.go:156] Writing cert to /home/hossein/.minikube/profiles/minikube/proxy-client.crt ...
I0208 10:25:52.451835  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/proxy-client.crt: {Name:mkb4e33287cb08055ad8898e662fd61e2faaacbd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:52.452082  539151 crypto.go:164] Writing key to /home/hossein/.minikube/profiles/minikube/proxy-client.key ...
I0208 10:25:52.452094  539151 lock.go:35] WriteFile acquiring /home/hossein/.minikube/profiles/minikube/proxy-client.key: {Name:mk13b785721d367ae10dc34184e71cc9ddb66f05 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:25:52.455412  539151 certs.go:484] found cert: /home/hossein/.minikube/certs/ca-key.pem (1675 bytes)
I0208 10:25:52.455719  539151 certs.go:484] found cert: /home/hossein/.minikube/certs/ca.pem (1078 bytes)
I0208 10:25:52.455794  539151 certs.go:484] found cert: /home/hossein/.minikube/certs/cert.pem (1123 bytes)
I0208 10:25:52.456049  539151 certs.go:484] found cert: /home/hossein/.minikube/certs/key.pem (1679 bytes)
I0208 10:25:52.457676  539151 ssh_runner.go:362] scp /home/hossein/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0208 10:25:52.510982  539151 ssh_runner.go:362] scp /home/hossein/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0208 10:25:52.560576  539151 ssh_runner.go:362] scp /home/hossein/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0208 10:25:52.613731  539151 ssh_runner.go:362] scp /home/hossein/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0208 10:25:52.663063  539151 ssh_runner.go:362] scp /home/hossein/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0208 10:25:52.715263  539151 ssh_runner.go:362] scp /home/hossein/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0208 10:25:52.764871  539151 ssh_runner.go:362] scp /home/hossein/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0208 10:25:52.818535  539151 ssh_runner.go:362] scp /home/hossein/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0208 10:25:52.876714  539151 ssh_runner.go:362] scp /home/hossein/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0208 10:25:52.943892  539151 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I0208 10:25:52.994750  539151 ssh_runner.go:195] Run: openssl version
I0208 10:25:53.011496  539151 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0208 10:25:53.032063  539151 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0208 10:25:53.039340  539151 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb  6 16:44 /usr/share/ca-certificates/minikubeCA.pem
I0208 10:25:53.039437  539151 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0208 10:25:53.053972  539151 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0208 10:25:53.076494  539151 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0208 10:25:53.084619  539151 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0208 10:25:53.084681  539151 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hossein:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0208 10:25:53.084838  539151 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0208 10:25:53.118355  539151 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0208 10:25:53.136581  539151 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0208 10:25:53.155726  539151 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0208 10:25:53.155793  539151 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0208 10:25:53.183603  539151 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0208 10:25:53.183619  539151 kubeadm.go:157] found existing configuration files:

I0208 10:25:53.183691  539151 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0208 10:25:53.202619  539151 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0208 10:25:53.202706  539151 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0208 10:25:53.220023  539151 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0208 10:25:53.238476  539151 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0208 10:25:53.238539  539151 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0208 10:25:53.256198  539151 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0208 10:25:53.276811  539151 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0208 10:25:53.276899  539151 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0208 10:25:53.297358  539151 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0208 10:25:53.316395  539151 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0208 10:25:53.316479  539151 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0208 10:25:53.333464  539151 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0208 10:25:53.486655  539151 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0208 10:25:53.500812  539151 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.8.0-41-generic\n", err: exit status 1
I0208 10:25:53.627661  539151 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0208 10:26:16.802695  539151 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0208 10:26:16.802846  539151 kubeadm.go:310] [preflight] Running pre-flight checks
I0208 10:26:16.803044  539151 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I0208 10:26:16.803200  539151 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.8.0-41-generic[0m
I0208 10:26:16.803286  539151 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I0208 10:26:16.803416  539151 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0208 10:26:16.803515  539151 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0208 10:26:16.803635  539151 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0208 10:26:16.803746  539151 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0208 10:26:16.803879  539151 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0208 10:26:16.803989  539151 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0208 10:26:16.804083  539151 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0208 10:26:16.804178  539151 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0208 10:26:16.804367  539151 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0208 10:26:16.804622  539151 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0208 10:26:16.804849  539151 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0208 10:26:16.804996  539151 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0208 10:26:16.815354  539151 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0208 10:26:16.815584  539151 kubeadm.go:310] [certs] Using existing ca certificate authority
I0208 10:26:16.815719  539151 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0208 10:26:16.815870  539151 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0208 10:26:16.815996  539151 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0208 10:26:16.816119  539151 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0208 10:26:16.816252  539151 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0208 10:26:16.816358  539151 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0208 10:26:16.816548  539151 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0208 10:26:16.816636  539151 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0208 10:26:16.816868  539151 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0208 10:26:16.817089  539151 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0208 10:26:16.817276  539151 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0208 10:26:16.817434  539151 kubeadm.go:310] [certs] Generating "sa" key and public key
I0208 10:26:16.817606  539151 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0208 10:26:16.817753  539151 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0208 10:26:16.817908  539151 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0208 10:26:16.818042  539151 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0208 10:26:16.818187  539151 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0208 10:26:16.818305  539151 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0208 10:26:16.818454  539151 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0208 10:26:16.818617  539151 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0208 10:26:16.827844  539151 out.go:235]     ‚ñ™ Booting up control plane ...
I0208 10:26:16.828252  539151 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0208 10:26:16.828528  539151 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0208 10:26:16.828761  539151 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0208 10:26:16.829081  539151 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0208 10:26:16.829382  539151 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0208 10:26:16.829492  539151 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0208 10:26:16.829934  539151 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0208 10:26:16.830254  539151 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0208 10:26:16.830418  539151 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001725466s
I0208 10:26:16.830623  539151 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0208 10:26:16.831749  539151 kubeadm.go:310] [api-check] The API server is healthy after 11.003610982s
I0208 10:26:16.832128  539151 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0208 10:26:16.832480  539151 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0208 10:26:16.832669  539151 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0208 10:26:16.833431  539151 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0208 10:26:16.833565  539151 kubeadm.go:310] [bootstrap-token] Using token: h1l0mb.mdsgnyte6j13c1z1
I0208 10:26:16.843994  539151 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0208 10:26:16.844563  539151 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0208 10:26:16.844995  539151 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0208 10:26:16.845512  539151 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0208 10:26:16.846004  539151 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0208 10:26:16.846496  539151 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0208 10:26:16.846813  539151 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0208 10:26:16.847318  539151 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0208 10:26:16.847481  539151 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0208 10:26:16.847698  539151 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0208 10:26:16.847728  539151 kubeadm.go:310] 
I0208 10:26:16.847982  539151 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0208 10:26:16.847993  539151 kubeadm.go:310] 
I0208 10:26:16.848403  539151 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0208 10:26:16.848417  539151 kubeadm.go:310] 
I0208 10:26:16.848589  539151 kubeadm.go:310]   mkdir -p $HOME/.kube
I0208 10:26:16.848873  539151 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0208 10:26:16.849154  539151 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0208 10:26:16.849227  539151 kubeadm.go:310] 
I0208 10:26:16.849438  539151 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0208 10:26:16.849448  539151 kubeadm.go:310] 
I0208 10:26:16.849686  539151 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0208 10:26:16.849711  539151 kubeadm.go:310] 
I0208 10:26:16.850022  539151 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0208 10:26:16.850490  539151 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0208 10:26:16.850803  539151 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0208 10:26:16.850833  539151 kubeadm.go:310] 
I0208 10:26:16.851284  539151 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0208 10:26:16.851526  539151 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0208 10:26:16.851536  539151 kubeadm.go:310] 
I0208 10:26:16.851771  539151 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token h1l0mb.mdsgnyte6j13c1z1 \
I0208 10:26:16.852030  539151 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:0b9bd0da1919de94f19d970ac35cd1b65af15b57f753071808047b2d1487d6ac \
I0208 10:26:16.852091  539151 kubeadm.go:310] 	--control-plane 
I0208 10:26:16.852098  539151 kubeadm.go:310] 
I0208 10:26:16.852385  539151 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0208 10:26:16.852404  539151 kubeadm.go:310] 
I0208 10:26:16.852602  539151 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token h1l0mb.mdsgnyte6j13c1z1 \
I0208 10:26:16.852942  539151 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:0b9bd0da1919de94f19d970ac35cd1b65af15b57f753071808047b2d1487d6ac 
I0208 10:26:16.852954  539151 cni.go:84] Creating CNI manager for ""
I0208 10:26:16.852975  539151 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0208 10:26:16.858793  539151 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0208 10:26:16.862813  539151 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0208 10:26:16.891842  539151 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0208 10:26:16.933300  539151 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0208 10:26:16.933410  539151 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0208 10:26:16.933429  539151 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_02_08T10_26_16_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0208 10:26:17.331025  539151 kubeadm.go:1113] duration metric: took 397.733173ms to wait for elevateKubeSystemPrivileges
I0208 10:26:17.331206  539151 ops.go:34] apiserver oom_adj: -16
I0208 10:26:17.331221  539151 kubeadm.go:394] duration metric: took 24.246544134s to StartCluster
I0208 10:26:17.331243  539151 settings.go:142] acquiring lock: {Name:mkbdf94cd9ce04a8fa5db82587208bb0684f0b82 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:26:17.340120  539151 settings.go:150] Updating kubeconfig:  /home/hossein/.kube/config
I0208 10:26:17.344428  539151 lock.go:35] WriteFile acquiring /home/hossein/.kube/config: {Name:mk9f251cc676dfdf8248fdff39969df38ea6a9b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0208 10:26:17.344812  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0208 10:26:17.344858  539151 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0208 10:26:17.344972  539151 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0208 10:26:17.345146  539151 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0208 10:26:17.345219  539151 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0208 10:26:17.345277  539151 host.go:66] Checking if "minikube" exists ...
I0208 10:26:17.345602  539151 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0208 10:26:17.346129  539151 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0208 10:26:17.346183  539151 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0208 10:26:17.354681  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:26:17.354683  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:26:17.356194  539151 out.go:177] üîé  Verifying Kubernetes components...
I0208 10:26:17.366844  539151 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0208 10:26:17.446865  539151 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0208 10:26:17.456816  539151 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0208 10:26:17.456836  539151 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0208 10:26:17.456940  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:26:17.499047  539151 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0208 10:26:17.499113  539151 host.go:66] Checking if "minikube" exists ...
I0208 10:26:17.500474  539151 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0208 10:26:17.548968  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:26:17.593856  539151 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0208 10:26:17.593878  539151 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0208 10:26:17.594020  539151 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0208 10:26:17.665763  539151 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32813 SSHKeyPath:/home/hossein/.minikube/machines/minikube/id_rsa Username:docker}
I0208 10:26:17.779719  539151 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0208 10:26:17.893433  539151 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0208 10:26:18.042860  539151 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0208 10:26:18.064202  539151 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0208 10:26:18.547744  539151 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0208 10:26:18.550977  539151 api_server.go:52] waiting for apiserver process to appear ...
I0208 10:26:18.551051  539151 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0208 10:26:18.867068  539151 api_server.go:72] duration metric: took 1.522165176s to wait for apiserver process to appear ...
I0208 10:26:18.867091  539151 api_server.go:88] waiting for apiserver healthz status ...
I0208 10:26:18.867122  539151 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0208 10:26:18.892206  539151 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0208 10:26:18.894539  539151 api_server.go:141] control plane version: v1.32.0
I0208 10:26:18.894577  539151 api_server.go:131] duration metric: took 27.472387ms to wait for apiserver health ...
I0208 10:26:18.894595  539151 system_pods.go:43] waiting for kube-system pods to appear ...
I0208 10:26:18.908290  539151 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0208 10:26:18.910220  539151 system_pods.go:59] 5 kube-system pods found
I0208 10:26:18.910250  539151 system_pods.go:61] "etcd-minikube" [b2d1e17f-8e2a-4d46-b867-de157608bc8b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0208 10:26:18.910263  539151 system_pods.go:61] "kube-apiserver-minikube" [96121c63-e670-4d9f-bb81-aa101489da72] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0208 10:26:18.910273  539151 system_pods.go:61] "kube-controller-manager-minikube" [9dae1183-9ccb-4518-83c2-fde75f7a492f] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0208 10:26:18.910282  539151 system_pods.go:61] "kube-scheduler-minikube" [2cd757be-9bed-41e5-a271-4074a5b9632b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0208 10:26:18.910289  539151 system_pods.go:61] "storage-provisioner" [89476291-0195-42e6-80e3-a574e76532c7] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0208 10:26:18.910297  539151 system_pods.go:74] duration metric: took 15.694469ms to wait for pod list to return data ...
I0208 10:26:18.910313  539151 kubeadm.go:582] duration metric: took 1.565418498s to wait for: map[apiserver:true system_pods:true]
I0208 10:26:18.910329  539151 node_conditions.go:102] verifying NodePressure condition ...
I0208 10:26:18.916908  539151 addons.go:514] duration metric: took 1.571935225s for enable addons: enabled=[storage-provisioner default-storageclass]
I0208 10:26:18.919309  539151 node_conditions.go:122] node storage ephemeral capacity is 119332656Ki
I0208 10:26:18.919363  539151 node_conditions.go:123] node cpu capacity is 4
I0208 10:26:18.919386  539151 node_conditions.go:105] duration metric: took 9.049504ms to run NodePressure ...
I0208 10:26:18.919411  539151 start.go:241] waiting for startup goroutines ...
I0208 10:26:19.058660  539151 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0208 10:26:19.058761  539151 start.go:246] waiting for cluster config update ...
I0208 10:26:19.058819  539151 start.go:255] writing updated cluster config ...
I0208 10:26:19.060044  539151 ssh_runner.go:195] Run: rm -f paused
I0208 10:26:19.362268  539151 start.go:600] kubectl: 1.32.1, cluster: 1.32.0 (minor skew: 0)
I0208 10:26:19.372383  539151 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 08 06:53:13 minikube cri-dockerd[1501]: time="2025-02-08T06:53:13Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 08 06:53:13 minikube cri-dockerd[1501]: time="2025-02-08T06:53:13Z" level=info msg="Start cri-dockerd grpc backend"
Feb 08 06:53:13 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 08 06:56:06 minikube cri-dockerd[1501]: time="2025-02-08T06:56:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2216b2b1e66cd8c297e805645c1900529c5217d5aadeb59b1b31b7e0c8d3cd61/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Feb 08 06:56:06 minikube cri-dockerd[1501]: time="2025-02-08T06:56:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/701b195e2ea399a0054882912c749854e51c119f5f504fc742a3ae28cf258f23/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 08 06:56:06 minikube cri-dockerd[1501]: time="2025-02-08T06:56:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/278053f4094c7bccdf8880c93202d395f6cb2fe7e66196fa38bbfe32565ddad0/resolv.conf as [nameserver 192.168.49.1 options trust-ad ndots:0 edns0]"
Feb 08 06:56:06 minikube cri-dockerd[1501]: time="2025-02-08T06:56:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/628ca35c16b5d2b9adb1464c5f2fbaa55d0992f2103a2605d5b50e5f095a21a2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 08 06:56:22 minikube cri-dockerd[1501]: time="2025-02-08T06:56:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e05543becc19bc14d29431ad98b4b2b2e9720785dd58f87be7b03019fa6c8715/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 08 06:56:22 minikube cri-dockerd[1501]: time="2025-02-08T06:56:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46fd7513dcdc672e69d01cae7263445d786b2f8ef2e8863688fa55de3c6b2733/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 08 06:56:22 minikube cri-dockerd[1501]: time="2025-02-08T06:56:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1d9168c0a929b2c93b51ebb9b3b004280e7f3dc0b69e2dc5cddba85194888c3/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 08 06:56:23 minikube dockerd[1230]: time="2025-02-08T06:56:23.198981537Z" level=info msg="ignoring event" container=1746a99b7afa5c7c5c152aee96b8e163997ba541b2c35a868d191c3e8bfb5c52 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 06:56:26 minikube cri-dockerd[1501]: time="2025-02-08T06:56:26Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 08 08:02:58 minikube cri-dockerd[1501]: time="2025-02-08T08:02:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e8a61f770d4f8b60545a7f4c9c3ef60fd87f3bedac06658c1a689832a87d2908/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 08:02:58 minikube cri-dockerd[1501]: time="2025-02-08T08:02:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/92bc4c20b2a49c50ce576cf83a426f4c03da81ba193ca668c01b2d2ff93b431e/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 08:03:00 minikube dockerd[1230]: time="2025-02-08T08:03:00.321693112Z" level=warning msg="reference for unknown type: " digest="sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 08 08:03:11 minikube cri-dockerd[1501]: time="2025-02-08T08:03:11Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=>                                                 ]  781.8kB/25.36MB"
Feb 08 08:03:21 minikube cri-dockerd[1501]: time="2025-02-08T08:03:21Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [========>                                          ]  4.437MB/25.36MB"
Feb 08 08:03:31 minikube cri-dockerd[1501]: time="2025-02-08T08:03:31Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=============>                                     ]  7.049MB/25.36MB"
Feb 08 08:03:41 minikube cri-dockerd[1501]: time="2025-02-08T08:03:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [================>                                  ]  8.615MB/25.36MB"
Feb 08 08:03:51 minikube cri-dockerd[1501]: time="2025-02-08T08:03:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=====================>                             ]   10.7MB/25.36MB"
Feb 08 08:04:01 minikube cri-dockerd[1501]: time="2025-02-08T08:04:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=======================>                           ]  12.01MB/25.36MB"
Feb 08 08:04:11 minikube cri-dockerd[1501]: time="2025-02-08T08:04:11Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [==========================>                        ]  13.58MB/25.36MB"
Feb 08 08:04:21 minikube cri-dockerd[1501]: time="2025-02-08T08:04:21Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=============================>                     ]  14.88MB/25.36MB"
Feb 08 08:04:31 minikube cri-dockerd[1501]: time="2025-02-08T08:04:31Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=================================>                 ]  17.23MB/25.36MB"
Feb 08 08:04:41 minikube cri-dockerd[1501]: time="2025-02-08T08:04:41Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [======================================>            ]  19.32MB/25.36MB"
Feb 08 08:04:51 minikube cri-dockerd[1501]: time="2025-02-08T08:04:51Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [===========================================>       ]  21.93MB/25.36MB"
Feb 08 08:05:01 minikube cri-dockerd[1501]: time="2025-02-08T08:05:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [===============================================>   ]  24.02MB/25.36MB"
Feb 08 08:05:05 minikube cri-dockerd[1501]: time="2025-02-08T08:05:05Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 08 08:05:06 minikube cri-dockerd[1501]: time="2025-02-08T08:05:06Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 08 08:05:06 minikube dockerd[1230]: time="2025-02-08T08:05:06.469103082Z" level=info msg="ignoring event" container=7c29a26dd7117d791ee2ce870df50034836dee83fe9f5e8161a8c3ffe57837a5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 08:05:06 minikube dockerd[1230]: time="2025-02-08T08:05:06.737521506Z" level=info msg="ignoring event" container=a1df494941b689abf34e8c5a00e84c7886f38d37a3f8927df8cb00c98824d3ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 08:05:07 minikube dockerd[1230]: time="2025-02-08T08:05:07.870076946Z" level=info msg="ignoring event" container=e8a61f770d4f8b60545a7f4c9c3ef60fd87f3bedac06658c1a689832a87d2908 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 08:05:08 minikube dockerd[1230]: time="2025-02-08T08:05:08.894444358Z" level=info msg="ignoring event" container=92bc4c20b2a49c50ce576cf83a426f4c03da81ba193ca668c01b2d2ff93b431e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 08 08:07:07 minikube cri-dockerd[1501]: time="2025-02-08T08:07:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/740dc730a25fdd9de9428e09181b176c585e0ff4904956a37da0d5abdfce9abc/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 08 08:07:08 minikube dockerd[1230]: time="2025-02-08T08:07:08.713040885Z" level=warning msg="reference for unknown type: " digest="sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7" remote="registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Feb 08 08:07:20 minikube cri-dockerd[1501]: time="2025-02-08T08:07:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Downloading [=========>                                         ]  938.5kB/4.975MB"
Feb 08 08:07:30 minikube cri-dockerd[1501]: time="2025-02-08T08:07:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Downloading [===========================>                       ]  2.714MB/4.975MB"
Feb 08 08:07:40 minikube cri-dockerd[1501]: time="2025-02-08T08:07:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [===>                                               ]  2.141MB/34.91MB"
Feb 08 08:07:50 minikube cri-dockerd[1501]: time="2025-02-08T08:07:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [===>                                               ]  2.141MB/34.91MB"
Feb 08 08:08:00 minikube cri-dockerd[1501]: time="2025-02-08T08:08:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 3adb5d6ffffa: Downloading [=====================================>             ]  3.759MB/4.975MB"
Feb 08 08:08:10 minikube cri-dockerd[1501]: time="2025-02-08T08:08:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Downloading [==================>                                ]  322.5kB/860.9kB"
Feb 08 08:08:20 minikube cri-dockerd[1501]: time="2025-02-08T08:08:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 2241f3d77dfe: Download complete "
Feb 08 08:08:30 minikube cri-dockerd[1501]: time="2025-02-08T08:08:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [============================>                      ]  11.49MB/19.87MB"
Feb 08 08:08:40 minikube cri-dockerd[1501]: time="2025-02-08T08:08:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [===================================>               ]  14.21MB/19.87MB"
Feb 08 08:08:50 minikube cri-dockerd[1501]: time="2025-02-08T08:08:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Downloading [===========================================>       ]  17.34MB/19.87MB"
Feb 08 08:09:00 minikube cri-dockerd[1501]: time="2025-02-08T08:09:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e05b61fcb166: Download complete "
Feb 08 08:09:10 minikube cri-dockerd[1501]: time="2025-02-08T08:09:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Download complete "
Feb 08 08:09:20 minikube cri-dockerd[1501]: time="2025-02-08T08:09:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [============>                                      ]  5.222MB/20.98MB"
Feb 08 08:09:30 minikube cri-dockerd[1501]: time="2025-02-08T08:09:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=================>                                 ]  7.398MB/20.98MB"
Feb 08 08:09:40 minikube cri-dockerd[1501]: time="2025-02-08T08:09:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=====================>                             ]  8.929MB/20.98MB"
Feb 08 08:09:50 minikube cri-dockerd[1501]: time="2025-02-08T08:09:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=========================>                         ]  10.88MB/20.98MB"
Feb 08 08:10:00 minikube cri-dockerd[1501]: time="2025-02-08T08:10:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [==================================>                ]   23.9MB/34.91MB"
Feb 08 08:10:10 minikube cri-dockerd[1501]: time="2025-02-08T08:10:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [==================================>                ]  14.36MB/20.98MB"
Feb 08 08:10:20 minikube cri-dockerd[1501]: time="2025-02-08T08:10:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [=====================================>             ]  15.89MB/20.98MB"
Feb 08 08:10:30 minikube cri-dockerd[1501]: time="2025-02-08T08:10:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [=========================================>         ]   28.9MB/34.91MB"
Feb 08 08:10:40 minikube cri-dockerd[1501]: time="2025-02-08T08:10:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 024e3a95c093: Downloading [==============================================>    ]  19.58MB/20.98MB"
Feb 08 08:10:50 minikube cri-dockerd[1501]: time="2025-02-08T08:10:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [===============================================>   ]  32.83MB/34.91MB"
Feb 08 08:11:00 minikube cri-dockerd[1501]: time="2025-02-08T08:11:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [================================================>  ]  33.89MB/34.91MB"
Feb 08 08:11:10 minikube cri-dockerd[1501]: time="2025-02-08T08:11:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [============================================>      ]  17.66MB/19.87MB"
Feb 08 08:11:13 minikube cri-dockerd[1501]: time="2025-02-08T08:11:13Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
17999f5669701       registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7             2 minutes ago       Running             controller                0                   740dc730a25fd       ingress-nginx-controller-56d7c84fd4-tgkrf
a1df494941b68       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   8 minutes ago       Exited              patch                     0                   92bc4c20b2a49       ingress-nginx-admission-patch-krv8w
7c29a26dd7117       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   8 minutes ago       Exited              create                    0                   e8a61f770d4f8       ingress-nginx-admission-create-6ghv5
804e49956f3ab       6e38f40d628db                                                                                                                About an hour ago   Running             storage-provisioner       1                   e05543becc19b       storage-provisioner
017ad2aa5b3ca       c69fa2e9cbf5f                                                                                                                About an hour ago   Running             coredns                   0                   e1d9168c0a929       coredns-668d6bf9bc-v8fgm
801f90f32c432       040f9f8aac8cd                                                                                                                About an hour ago   Running             kube-proxy                0                   46fd7513dcdc6       kube-proxy-zzws4
1746a99b7afa5       6e38f40d628db                                                                                                                About an hour ago   Exited              storage-provisioner       0                   e05543becc19b       storage-provisioner
b806470970fd5       a9e7e6b294baf                                                                                                                About an hour ago   Running             etcd                      0                   628ca35c16b5d       etcd-minikube
30b18311cc9fe       a389e107f4ff1                                                                                                                About an hour ago   Running             kube-scheduler            0                   278053f4094c7       kube-scheduler-minikube
5fd3732a8d097       c2e17b8d0f4a3                                                                                                                About an hour ago   Running             kube-apiserver            0                   701b195e2ea39       kube-apiserver-minikube
e0f9db0c55b3b       8cab3d2a8bd0f                                                                                                                About an hour ago   Running             kube-controller-manager   0                   2216b2b1e66cd       kube-controller-manager-minikube


==> coredns [017ad2aa5b3c] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: no route to host
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:42555 - 62503 "HINFO IN 6992054143764462303.8101857819710952420. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.200798424s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_08T10_26_16_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 08 Feb 2025 06:56:13 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 08 Feb 2025 08:13:54 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 08 Feb 2025 08:11:26 +0000   Sat, 08 Feb 2025 06:56:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 08 Feb 2025 08:11:26 +0000   Sat, 08 Feb 2025 06:56:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 08 Feb 2025 08:11:26 +0000   Sat, 08 Feb 2025 06:56:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 08 Feb 2025 08:11:26 +0000   Sat, 08 Feb 2025 06:56:13 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  119332656Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3886500Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  119332656Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3886500Ki
  pods:               110
System Info:
  Machine ID:                 e723a7022b434aafb5dcaba04bd5ac65
  System UUID:                e51c86a5-a5c8-43d9-9517-df2e346ed40a
  Boot ID:                    c01b14ab-9ae4-40c1-a09e-57ee18b6a831
  Kernel Version:             6.8.0-41-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-tgkrf    100m (2%)     0 (0%)      90Mi (2%)        0 (0%)         10m
  kube-system                 coredns-668d6bf9bc-v8fgm                     100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     77m
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         77m
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-proxy-zzws4                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             260Mi (6%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Feb 8 04:43] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 04:50] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:07] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:09] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:13] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:14] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:15] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:17] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:20] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:21] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:25] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:30] kauditd_printk_skb: 25 callbacks suppressed
[Feb 8 05:38] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:43] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:44] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:46] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:47] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:54] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[  +6.399874] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[ +35.599420] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 05:55] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:02] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:06] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:11] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:15] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[ +16.399814] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:17] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:18] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[ +24.399466] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:19] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:23] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:28] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:33] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:37] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:39] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[  +8.799966] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:46] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:49] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:51] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:54] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:58] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 06:59] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:10] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:11] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:12] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:13] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:14] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:19] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:24] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:34] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:35] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:37] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[ +46.399340] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:38] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:43] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:53] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 07:57] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 08:03] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[Feb 8 08:10] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)
[ +21.599709] i2c_hid_acpi i2c-SMO91D0:00: i2c_hid_get_input: incomplete report (53/13568)


==> etcd [b806470970fd] <==
{"level":"info","ts":"2025-02-08T06:56:07.973628Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-08T06:56:07.973542Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-08T06:56:07.975103Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-08T06:56:07.975479Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-08T06:56:07.986043Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-08T06:56:07.986104Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-08T06:56:07.986265Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-08T06:56:07.986318Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-08T06:56:07.986579Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-08T06:56:07.988042Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-08T06:56:07.988362Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-08T07:06:10.076108Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":646}
{"level":"info","ts":"2025-02-08T07:06:10.095060Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":646,"took":"18.233659ms","hash":1771886279,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-02-08T07:06:10.095264Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1771886279,"revision":646,"compact-revision":-1}
{"level":"info","ts":"2025-02-08T07:11:10.093634Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":885}
{"level":"info","ts":"2025-02-08T07:11:10.099889Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":885,"took":"5.884721ms","hash":243319328,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":913408,"current-db-size-in-use":"913 kB"}
{"level":"info","ts":"2025-02-08T07:11:10.099945Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":243319328,"revision":885,"compact-revision":646}
{"level":"info","ts":"2025-02-08T07:16:10.116517Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1124}
{"level":"info","ts":"2025-02-08T07:16:10.131496Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1124,"took":"14.064907ms","hash":2139273175,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2025-02-08T07:16:10.131704Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2139273175,"revision":1124,"compact-revision":885}
{"level":"info","ts":"2025-02-08T07:21:10.141064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1363}
{"level":"info","ts":"2025-02-08T07:21:10.155577Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1363,"took":"13.551313ms","hash":753869568,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2025-02-08T07:21:10.155685Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":753869568,"revision":1363,"compact-revision":1124}
{"level":"info","ts":"2025-02-08T07:26:10.171696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1602}
{"level":"info","ts":"2025-02-08T07:26:10.180491Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1602,"took":"7.480767ms","hash":4130490991,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2025-02-08T07:26:10.180596Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4130490991,"revision":1602,"compact-revision":1363}
{"level":"info","ts":"2025-02-08T07:31:10.192902Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1841}
{"level":"info","ts":"2025-02-08T07:31:10.205707Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1841,"took":"11.987772ms","hash":4229156691,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2025-02-08T07:31:10.205974Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4229156691,"revision":1841,"compact-revision":1602}
{"level":"info","ts":"2025-02-08T07:36:10.208655Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2081}
{"level":"info","ts":"2025-02-08T07:36:10.222536Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2081,"took":"12.976019ms","hash":2605175771,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2025-02-08T07:36:10.222736Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2605175771,"revision":2081,"compact-revision":1841}
{"level":"info","ts":"2025-02-08T07:41:10.225767Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2320}
{"level":"info","ts":"2025-02-08T07:41:10.240522Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2320,"took":"13.0259ms","hash":2707737274,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":892928,"current-db-size-in-use":"893 kB"}
{"level":"info","ts":"2025-02-08T07:41:10.240658Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2707737274,"revision":2320,"compact-revision":2081}
{"level":"info","ts":"2025-02-08T07:46:10.248665Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2559}
{"level":"info","ts":"2025-02-08T07:46:10.262420Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2559,"took":"12.931861ms","hash":1709805475,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":892928,"current-db-size-in-use":"893 kB"}
{"level":"info","ts":"2025-02-08T07:46:10.262549Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1709805475,"revision":2559,"compact-revision":2320}
{"level":"info","ts":"2025-02-08T07:51:10.266606Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2798}
{"level":"info","ts":"2025-02-08T07:51:10.274543Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2798,"took":"7.217304ms","hash":1123239123,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2025-02-08T07:51:10.274668Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1123239123,"revision":2798,"compact-revision":2559}
{"level":"info","ts":"2025-02-08T07:56:10.289598Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3037}
{"level":"info","ts":"2025-02-08T07:56:10.299884Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3037,"took":"9.320291ms","hash":567150681,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2025-02-08T07:56:10.300018Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":567150681,"revision":3037,"compact-revision":2798}
{"level":"info","ts":"2025-02-08T08:01:10.313039Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3276}
{"level":"info","ts":"2025-02-08T08:01:10.335025Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3276,"took":"21.18686ms","hash":3435519811,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2025-02-08T08:01:10.335079Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3435519811,"revision":3276,"compact-revision":3037}
{"level":"info","ts":"2025-02-08T08:05:05.851936Z","caller":"traceutil/trace.go:171","msg":"trace[646429537] linearizableReadLoop","detail":"{readStateIndex:4621; appliedIndex:4620; }","duration":"158.471731ms","start":"2025-02-08T08:05:05.602450Z","end":"2025-02-08T08:05:05.760922Z","steps":["trace[646429537] 'read index received'  (duration: 61.089903ms)","trace[646429537] 'applied index is now lower than readState.Index'  (duration: 97.380144ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-08T08:05:05.881135Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.168052ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-08T08:05:05.908598Z","caller":"traceutil/trace.go:171","msg":"trace[583535165] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:3770; }","duration":"306.141399ms","start":"2025-02-08T08:05:05.602397Z","end":"2025-02-08T08:05:05.908538Z","steps":["trace[583535165] 'agreement among raft nodes before linearized reading'  (duration: 159.337601ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-08T08:05:05.908815Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-08T08:05:05.602374Z","time spent":"306.373758ms","remote":"127.0.0.1:36446","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/pods\" limit:1 "}
{"level":"warn","ts":"2025-02-08T08:05:05.924342Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"292.830699ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-02-08T08:05:05.924642Z","caller":"traceutil/trace.go:171","msg":"trace[683215367] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:3771; }","duration":"293.147375ms","start":"2025-02-08T08:05:05.631383Z","end":"2025-02-08T08:05:05.924530Z","steps":["trace[683215367] 'agreement among raft nodes before linearized reading'  (duration: 252.270729ms)","trace[683215367] 'count revisions from in-memory index tree'  (duration: 40.52787ms)"],"step_count":2}
{"level":"info","ts":"2025-02-08T08:06:10.325488Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3517}
{"level":"info","ts":"2025-02-08T08:06:10.344800Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3517,"took":"18.576713ms","hash":1580834359,"current-db-size-bytes":1576960,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1527808,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-02-08T08:06:10.344855Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1580834359,"revision":3517,"compact-revision":3276}
{"level":"info","ts":"2025-02-08T08:11:10.335292Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3843}
{"level":"info","ts":"2025-02-08T08:11:10.348303Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3843,"took":"12.622139ms","hash":1692354250,"current-db-size-bytes":1576960,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-02-08T08:11:10.348371Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1692354250,"revision":3843,"compact-revision":3517}
{"level":"info","ts":"2025-02-08T08:11:14.618130Z","caller":"traceutil/trace.go:171","msg":"trace[1417598174] transaction","detail":"{read_only:false; response_revision:4086; number_of_response:1; }","duration":"245.028821ms","start":"2025-02-08T08:11:14.373025Z","end":"2025-02-08T08:11:14.618054Z","steps":["trace[1417598174] 'process raft request'  (duration: 244.711326ms)"],"step_count":1}


==> kernel <==
 08:13:54 up 1 day, 19:36,  0 users,  load average: 2.02, 1.11, 0.86
Linux minikube 6.8.0-41-generic #41-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug  2 20:41:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [5fd3732a8d09] <==
I0208 06:56:12.791600       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0208 06:56:12.791612       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0208 06:56:12.791782       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0208 06:56:12.791810       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0208 06:56:12.791844       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0208 06:56:12.791856       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0208 06:56:12.793881       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0208 06:56:12.794265       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0208 06:56:12.794285       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0208 06:56:12.794847       1 controller.go:142] Starting OpenAPI controller
I0208 06:56:12.795145       1 controller.go:90] Starting OpenAPI V3 controller
I0208 06:56:12.795468       1 naming_controller.go:294] Starting NamingConditionController
I0208 06:56:12.795827       1 establishing_controller.go:81] Starting EstablishingController
I0208 06:56:12.796106       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0208 06:56:12.796152       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0208 06:56:12.796198       1 crd_finalizer.go:269] Starting CRDFinalizer
I0208 06:56:12.797797       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0208 06:56:12.798329       1 controller.go:119] Starting legacy_token_tracking_controller
I0208 06:56:12.798355       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0208 06:56:12.802464       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0208 06:56:12.802627       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0208 06:56:13.089219       1 cache.go:39] Caches are synced for LocalAvailability controller
I0208 06:56:13.089253       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0208 06:56:13.089283       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0208 06:56:13.089609       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0208 06:56:13.092658       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0208 06:56:13.092797       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0208 06:56:13.092815       1 aggregator.go:171] initial CRD sync complete...
I0208 06:56:13.093128       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0208 06:56:13.093143       1 autoregister_controller.go:144] Starting autoregister controller
I0208 06:56:13.093412       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0208 06:56:13.093508       1 cache.go:39] Caches are synced for autoregister controller
I0208 06:56:13.094329       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0208 06:56:13.098461       1 shared_informer.go:320] Caches are synced for configmaps
I0208 06:56:13.122976       1 shared_informer.go:320] Caches are synced for node_authorizer
E0208 06:56:13.157463       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0208 06:56:13.171487       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0208 06:56:13.171520       1 policy_source.go:240] refreshing policies
I0208 06:56:13.202225       1 controller.go:615] quota admission added evaluator for: namespaces
E0208 06:56:13.213028       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0208 06:56:13.237563       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0208 06:56:13.435919       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0208 06:56:13.812986       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0208 06:56:13.828067       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0208 06:56:13.828096       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0208 06:56:15.097572       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0208 06:56:15.170214       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0208 06:56:15.330258       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0208 06:56:15.344938       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0208 06:56:15.346757       1 controller.go:615] quota admission added evaluator for: endpoints
I0208 06:56:15.357082       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0208 06:56:15.987223       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0208 06:56:16.224137       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0208 06:56:16.255994       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0208 06:56:16.281601       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0208 06:56:21.344533       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0208 06:56:21.485423       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0208 08:02:56.800689       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.108.12.202"}
I0208 08:02:56.846449       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.110.5.106"}
I0208 08:02:56.914401       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [e0f9db0c55b3] <==
I0208 06:56:20.602262       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0208 06:56:20.605435       1 shared_informer.go:320] Caches are synced for HPA
I0208 06:56:20.607723       1 shared_informer.go:320] Caches are synced for cronjob
I0208 06:56:20.611051       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0208 06:56:20.613766       1 shared_informer.go:320] Caches are synced for taint
I0208 06:56:20.613963       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0208 06:56:20.614115       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0208 06:56:20.614282       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0208 06:56:20.614829       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0208 06:56:20.614876       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 06:56:20.615486       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 06:56:20.642561       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 06:56:21.195951       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 06:56:21.706257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="345.360261ms"
I0208 06:56:21.781636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="75.154417ms"
I0208 06:56:21.781798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="93.701¬µs"
I0208 06:56:23.981469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="100.926¬µs"
I0208 06:56:25.561761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="24.929618ms"
I0208 06:56:25.562136       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="162.626¬µs"
I0208 06:56:26.736673       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 06:59:50.176349       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:04:57.372626       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:10:04.203237       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:15:09.967971       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:20:16.279990       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:25:22.546056       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:30:28.504880       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:35:34.748123       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:40:40.442119       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:45:47.351581       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:50:52.572444       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 07:55:59.331544       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 08:01:04.500231       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 08:02:56.924348       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0208 08:02:56.943383       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0208 08:02:56.963342       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:02:56.974578       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:02:56.996524       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:02:57.029501       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:02:57.057821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="128.328229ms"
I0208 08:02:57.058999       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:02:57.059053       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:02:57.072890       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:02:57.103911       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="45.991232ms"
I0208 08:02:57.104182       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="156.613¬µs"
I0208 08:02:57.111885       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="133.354¬µs"
I0208 08:02:57.140305       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:05:06.599657       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:05:07.739228       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:05:07.990048       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:05:09.022401       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:05:09.036892       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0208 08:05:09.041901       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:05:10.062096       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:05:10.085669       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0208 08:05:19.276214       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 08:11:15.641926       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="187.269¬µs"
I0208 08:11:26.631212       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0208 08:11:26.705136       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="28.000664ms"
I0208 08:11:26.705792       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="175.293¬µs"


==> kube-proxy [801f90f32c43] <==
I0208 06:56:23.341876       1 server_linux.go:66] "Using iptables proxy"
I0208 06:56:23.497281       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0208 06:56:23.497741       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0208 06:56:23.540962       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0208 06:56:23.541050       1 server_linux.go:170] "Using iptables Proxier"
I0208 06:56:23.550006       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0208 06:56:23.554215       1 server.go:497] "Version info" version="v1.32.0"
I0208 06:56:23.554350       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0208 06:56:23.561108       1 config.go:199] "Starting service config controller"
I0208 06:56:23.564381       1 config.go:105] "Starting endpoint slice config controller"
I0208 06:56:23.564531       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0208 06:56:23.564564       1 shared_informer.go:313] Waiting for caches to sync for service config
I0208 06:56:23.564783       1 config.go:329] "Starting node config controller"
I0208 06:56:23.564805       1 shared_informer.go:313] Waiting for caches to sync for node config
I0208 06:56:23.665049       1 shared_informer.go:320] Caches are synced for node config
I0208 06:56:23.665049       1 shared_informer.go:320] Caches are synced for service config
I0208 06:56:23.665110       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [30b18311cc9f] <==
W0208 06:56:13.561674       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0208 06:56:13.590745       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0208 06:56:13.590802       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0208 06:56:13.595877       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0208 06:56:13.595943       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0208 06:56:13.596500       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0208 06:56:13.596815       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0208 06:56:13.600829       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0208 06:56:13.601009       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0208 06:56:13.604589       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0208 06:56:13.604874       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.605821       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0208 06:56:13.606450       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606519       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0208 06:56:13.607440       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606531       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0208 06:56:13.607779       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606257       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0208 06:56:13.607884       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606554       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0208 06:56:13.608076       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606653       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:13.608180       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606744       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0208 06:56:13.608237       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606782       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:13.608285       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606834       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:13.608332       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.607033       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0208 06:56:13.608361       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.607115       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0208 06:56:13.608394       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.607061       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0208 06:56:13.608436       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.606030       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:13.608473       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:13.607368       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:13.608508       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.437718       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0208 06:56:14.438715       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.467902       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:14.468022       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.488898       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0208 06:56:14.488995       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.500632       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:14.500717       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.663486       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0208 06:56:14.663564       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.686648       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0208 06:56:14.686706       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.695567       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0208 06:56:14.695636       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.707714       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0208 06:56:14.707799       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0208 06:56:14.719430       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0208 06:56:14.719620       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0208 06:56:14.756020       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0208 06:56:14.756100       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0208 06:56:16.496541       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 08 06:56:17 minikube kubelet[3111]: I0208 06:56:17.489700    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.48966138 podStartE2EDuration="1.48966138s" podCreationTimestamp="2025-02-08 06:56:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:17.424537389 +0000 UTC m=+1.273486881" watchObservedRunningTime="2025-02-08 06:56:17.48966138 +0000 UTC m=+1.338610829"
Feb 08 06:56:17 minikube kubelet[3111]: I0208 06:56:17.558366    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.558336586 podStartE2EDuration="1.558336586s" podCreationTimestamp="2025-02-08 06:56:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:17.491018026 +0000 UTC m=+1.339967491" watchObservedRunningTime="2025-02-08 06:56:17.558336586 +0000 UTC m=+1.407286059"
Feb 08 06:56:17 minikube kubelet[3111]: I0208 06:56:17.601104    3111 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Feb 08 06:56:17 minikube kubelet[3111]: I0208 06:56:17.613779    3111 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Feb 08 06:56:17 minikube kubelet[3111]: E0208 06:56:17.657955    3111 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Feb 08 06:56:17 minikube kubelet[3111]: I0208 06:56:17.659767    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.6591000930000002 podStartE2EDuration="1.659100093s" podCreationTimestamp="2025-02-08 06:56:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:17.552013473 +0000 UTC m=+1.400962967" watchObservedRunningTime="2025-02-08 06:56:17.659100093 +0000 UTC m=+1.508049571"
Feb 08 06:56:17 minikube kubelet[3111]: E0208 06:56:17.685988    3111 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Feb 08 06:56:20 minikube kubelet[3111]: I0208 06:56:20.729307    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/89476291-0195-42e6-80e3-a574e76532c7-tmp\") pod \"storage-provisioner\" (UID: \"89476291-0195-42e6-80e3-a574e76532c7\") " pod="kube-system/storage-provisioner"
Feb 08 06:56:20 minikube kubelet[3111]: I0208 06:56:20.729394    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f9qrf\" (UniqueName: \"kubernetes.io/projected/89476291-0195-42e6-80e3-a574e76532c7-kube-api-access-f9qrf\") pod \"storage-provisioner\" (UID: \"89476291-0195-42e6-80e3-a574e76532c7\") " pod="kube-system/storage-provisioner"
Feb 08 06:56:20 minikube kubelet[3111]: E0208 06:56:20.844784    3111 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 08 06:56:20 minikube kubelet[3111]: E0208 06:56:20.844865    3111 projected.go:194] Error preparing data for projected volume kube-api-access-f9qrf for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Feb 08 06:56:20 minikube kubelet[3111]: E0208 06:56:20.845071    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/89476291-0195-42e6-80e3-a574e76532c7-kube-api-access-f9qrf podName:89476291-0195-42e6-80e3-a574e76532c7 nodeName:}" failed. No retries permitted until 2025-02-08 06:56:21.344973962 +0000 UTC m=+5.193923455 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-f9qrf" (UniqueName: "kubernetes.io/projected/89476291-0195-42e6-80e3-a574e76532c7-kube-api-access-f9qrf") pod "storage-provisioner" (UID: "89476291-0195-42e6-80e3-a574e76532c7") : configmap "kube-root-ca.crt" not found
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.636509    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/84cbf409-c408-42ab-8589-1595ba23cbae-xtables-lock\") pod \"kube-proxy-zzws4\" (UID: \"84cbf409-c408-42ab-8589-1595ba23cbae\") " pod="kube-system/kube-proxy-zzws4"
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.636604    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/84cbf409-c408-42ab-8589-1595ba23cbae-lib-modules\") pod \"kube-proxy-zzws4\" (UID: \"84cbf409-c408-42ab-8589-1595ba23cbae\") " pod="kube-system/kube-proxy-zzws4"
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.636664    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vw9qg\" (UniqueName: \"kubernetes.io/projected/84cbf409-c408-42ab-8589-1595ba23cbae-kube-api-access-vw9qg\") pod \"kube-proxy-zzws4\" (UID: \"84cbf409-c408-42ab-8589-1595ba23cbae\") " pod="kube-system/kube-proxy-zzws4"
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.636723    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/84cbf409-c408-42ab-8589-1595ba23cbae-kube-proxy\") pod \"kube-proxy-zzws4\" (UID: \"84cbf409-c408-42ab-8589-1595ba23cbae\") " pod="kube-system/kube-proxy-zzws4"
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.737433    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/0dd9f6de-5400-4ff2-bee1-897d14d2fc33-config-volume\") pod \"coredns-668d6bf9bc-v8fgm\" (UID: \"0dd9f6de-5400-4ff2-bee1-897d14d2fc33\") " pod="kube-system/coredns-668d6bf9bc-v8fgm"
Feb 08 06:56:21 minikube kubelet[3111]: I0208 06:56:21.737656    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2mkn7\" (UniqueName: \"kubernetes.io/projected/0dd9f6de-5400-4ff2-bee1-897d14d2fc33-kube-api-access-2mkn7\") pod \"coredns-668d6bf9bc-v8fgm\" (UID: \"0dd9f6de-5400-4ff2-bee1-897d14d2fc33\") " pod="kube-system/coredns-668d6bf9bc-v8fgm"
Feb 08 06:56:22 minikube kubelet[3111]: I0208 06:56:22.867762    3111 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e1d9168c0a929b2c93b51ebb9b3b004280e7f3dc0b69e2dc5cddba85194888c3"
Feb 08 06:56:22 minikube kubelet[3111]: I0208 06:56:22.951018    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=4.950979748 podStartE2EDuration="4.950979748s" podCreationTimestamp="2025-02-08 06:56:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:22.949898438 +0000 UTC m=+6.798847914" watchObservedRunningTime="2025-02-08 06:56:22.950979748 +0000 UTC m=+6.799929199"
Feb 08 06:56:23 minikube kubelet[3111]: I0208 06:56:23.970593    3111 scope.go:117] "RemoveContainer" containerID="1746a99b7afa5c7c5c152aee96b8e163997ba541b2c35a868d191c3e8bfb5c52"
Feb 08 06:56:24 minikube kubelet[3111]: I0208 06:56:24.040206    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-v8fgm" podStartSLOduration=3.040155849 podStartE2EDuration="3.040155849s" podCreationTimestamp="2025-02-08 06:56:21 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:23.983283427 +0000 UTC m=+7.832232900" watchObservedRunningTime="2025-02-08 06:56:24.040155849 +0000 UTC m=+7.889105314"
Feb 08 06:56:24 minikube kubelet[3111]: I0208 06:56:24.081613    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-zzws4" podStartSLOduration=3.081580501 podStartE2EDuration="3.081580501s" podCreationTimestamp="2025-02-08 06:56:21 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-08 06:56:24.080864025 +0000 UTC m=+7.929813498" watchObservedRunningTime="2025-02-08 06:56:24.081580501 +0000 UTC m=+7.930529958"
Feb 08 06:56:24 minikube kubelet[3111]: I0208 06:56:24.996144    3111 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 08 06:56:26 minikube kubelet[3111]: I0208 06:56:26.702402    3111 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 08 06:56:26 minikube kubelet[3111]: I0208 06:56:26.707894    3111 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 08 08:02:57 minikube kubelet[3111]: I0208 08:02:57.178114    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mgbgg\" (UniqueName: \"kubernetes.io/projected/1e49bfc6-7094-4c64-9c5d-9d51341849f0-kube-api-access-mgbgg\") pod \"ingress-nginx-admission-patch-krv8w\" (UID: \"1e49bfc6-7094-4c64-9c5d-9d51341849f0\") " pod="ingress-nginx/ingress-nginx-admission-patch-krv8w"
Feb 08 08:02:57 minikube kubelet[3111]: I0208 08:02:57.178342    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cdwwm\" (UniqueName: \"kubernetes.io/projected/4fa242f0-a46c-46d5-b0af-c279bc343134-kube-api-access-cdwwm\") pod \"ingress-nginx-controller-56d7c84fd4-tgkrf\" (UID: \"4fa242f0-a46c-46d5-b0af-c279bc343134\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tgkrf"
Feb 08 08:02:57 minikube kubelet[3111]: I0208 08:02:57.178427    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k7wkj\" (UniqueName: \"kubernetes.io/projected/2183fc78-7004-46bf-a5dc-4421842aa34d-kube-api-access-k7wkj\") pod \"ingress-nginx-admission-create-6ghv5\" (UID: \"2183fc78-7004-46bf-a5dc-4421842aa34d\") " pod="ingress-nginx/ingress-nginx-admission-create-6ghv5"
Feb 08 08:02:57 minikube kubelet[3111]: I0208 08:02:57.178489    3111 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert\") pod \"ingress-nginx-controller-56d7c84fd4-tgkrf\" (UID: \"4fa242f0-a46c-46d5-b0af-c279bc343134\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tgkrf"
Feb 08 08:02:57 minikube kubelet[3111]: E0208 08:02:57.280024    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:02:57 minikube kubelet[3111]: E0208 08:02:57.281368    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:02:57.781073567 +0000 UTC m=+4001.630023026 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:02:57 minikube kubelet[3111]: E0208 08:02:57.783581    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:02:57 minikube kubelet[3111]: E0208 08:02:57.783825    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:02:58.783748165 +0000 UTC m=+4002.632697720 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:02:58 minikube kubelet[3111]: E0208 08:02:58.791714    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:02:58 minikube kubelet[3111]: E0208 08:02:58.791862    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:03:00.791814105 +0000 UTC m=+4004.640763578 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:02:58 minikube kubelet[3111]: I0208 08:02:58.833640    3111 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e8a61f770d4f8b60545a7f4c9c3ef60fd87f3bedac06658c1a689832a87d2908"
Feb 08 08:02:58 minikube kubelet[3111]: I0208 08:02:58.848943    3111 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="92bc4c20b2a49c50ce576cf83a426f4c03da81ba193ca668c01b2d2ff93b431e"
Feb 08 08:03:00 minikube kubelet[3111]: E0208 08:03:00.808893    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:03:00 minikube kubelet[3111]: E0208 08:03:00.809242    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:03:04.809091563 +0000 UTC m=+4008.658041138 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:03:04 minikube kubelet[3111]: E0208 08:03:04.845308    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:03:04 minikube kubelet[3111]: E0208 08:03:04.845570    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:03:12.845490535 +0000 UTC m=+4016.694440070 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:03:12 minikube kubelet[3111]: E0208 08:03:12.909893    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:03:12 minikube kubelet[3111]: E0208 08:03:12.910010    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:03:28.909975368 +0000 UTC m=+4032.758924820 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:03:28 minikube kubelet[3111]: E0208 08:03:28.937259    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:03:28 minikube kubelet[3111]: E0208 08:03:28.937522    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:04:00.937439065 +0000 UTC m=+4064.786388580 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:04:01 minikube kubelet[3111]: E0208 08:04:01.030933    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:04:01 minikube kubelet[3111]: E0208 08:04:01.031065    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:05:05.031032167 +0000 UTC m=+4128.879981624 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:05:00 minikube kubelet[3111]: E0208 08:05:00.080913    3111 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tgkrf" podUID="4fa242f0-a46c-46d5-b0af-c279bc343134"
Feb 08 08:05:05 minikube kubelet[3111]: E0208 08:05:05.092959    3111 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 08 08:05:05 minikube kubelet[3111]: E0208 08:05:05.094781    3111 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert podName:4fa242f0-a46c-46d5-b0af-c279bc343134 nodeName:}" failed. No retries permitted until 2025-02-08 08:07:07.093981681 +0000 UTC m=+4250.942931143 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/4fa242f0-a46c-46d5-b0af-c279bc343134-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tgkrf" (UID: "4fa242f0-a46c-46d5-b0af-c279bc343134") : secret "ingress-nginx-admission" not found
Feb 08 08:05:08 minikube kubelet[3111]: I0208 08:05:08.121065    3111 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-k7wkj\" (UniqueName: \"kubernetes.io/projected/2183fc78-7004-46bf-a5dc-4421842aa34d-kube-api-access-k7wkj\") pod \"2183fc78-7004-46bf-a5dc-4421842aa34d\" (UID: \"2183fc78-7004-46bf-a5dc-4421842aa34d\") "
Feb 08 08:05:08 minikube kubelet[3111]: I0208 08:05:08.128079    3111 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2183fc78-7004-46bf-a5dc-4421842aa34d-kube-api-access-k7wkj" (OuterVolumeSpecName: "kube-api-access-k7wkj") pod "2183fc78-7004-46bf-a5dc-4421842aa34d" (UID: "2183fc78-7004-46bf-a5dc-4421842aa34d"). InnerVolumeSpecName "kube-api-access-k7wkj". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 08 08:05:08 minikube kubelet[3111]: I0208 08:05:08.221921    3111 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-k7wkj\" (UniqueName: \"kubernetes.io/projected/2183fc78-7004-46bf-a5dc-4421842aa34d-kube-api-access-k7wkj\") on node \"minikube\" DevicePath \"\""
Feb 08 08:05:08 minikube kubelet[3111]: I0208 08:05:08.736851    3111 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e8a61f770d4f8b60545a7f4c9c3ef60fd87f3bedac06658c1a689832a87d2908"
Feb 08 08:05:09 minikube kubelet[3111]: I0208 08:05:09.129672    3111 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mgbgg\" (UniqueName: \"kubernetes.io/projected/1e49bfc6-7094-4c64-9c5d-9d51341849f0-kube-api-access-mgbgg\") pod \"1e49bfc6-7094-4c64-9c5d-9d51341849f0\" (UID: \"1e49bfc6-7094-4c64-9c5d-9d51341849f0\") "
Feb 08 08:05:09 minikube kubelet[3111]: I0208 08:05:09.133392    3111 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1e49bfc6-7094-4c64-9c5d-9d51341849f0-kube-api-access-mgbgg" (OuterVolumeSpecName: "kube-api-access-mgbgg") pod "1e49bfc6-7094-4c64-9c5d-9d51341849f0" (UID: "1e49bfc6-7094-4c64-9c5d-9d51341849f0"). InnerVolumeSpecName "kube-api-access-mgbgg". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 08 08:05:09 minikube kubelet[3111]: I0208 08:05:09.230827    3111 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-mgbgg\" (UniqueName: \"kubernetes.io/projected/1e49bfc6-7094-4c64-9c5d-9d51341849f0-kube-api-access-mgbgg\") on node \"minikube\" DevicePath \"\""
Feb 08 08:05:09 minikube kubelet[3111]: I0208 08:05:09.769797    3111 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="92bc4c20b2a49c50ce576cf83a426f4c03da81ba193ca668c01b2d2ff93b431e"
Feb 08 08:11:26 minikube kubelet[3111]: I0208 08:11:26.678037    3111 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tgkrf" podStartSLOduration=264.79456519 podStartE2EDuration="8m30.677481748s" podCreationTimestamp="2025-02-08 08:02:56 +0000 UTC" firstStartedPulling="2025-02-08 08:07:08.018716267 +0000 UTC m=+4251.867665740" lastFinishedPulling="2025-02-08 08:11:13.901632717 +0000 UTC m=+4497.750582298" observedRunningTime="2025-02-08 08:11:15.6411217 +0000 UTC m=+4499.490071190" watchObservedRunningTime="2025-02-08 08:11:26.677481748 +0000 UTC m=+4510.526431200"


==> storage-provisioner [1746a99b7afa] <==
I0208 06:56:22.774042       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0208 06:56:23.167390       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: no route to host


==> storage-provisioner [804e49956f3a] <==
I0208 06:56:24.356821       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0208 06:56:24.387912       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0208 06:56:24.387992       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0208 06:56:24.417243       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0208 06:56:24.417493       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_228ee62a-27c8-4f86-84d3-bedf62960917!
I0208 06:56:24.417667       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"021f77e3-1fa8-4fcc-a513-811349c1012f", APIVersion:"v1", ResourceVersion:"410", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_228ee62a-27c8-4f86-84d3-bedf62960917 became leader
I0208 06:56:24.518582       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_228ee62a-27c8-4f86-84d3-bedf62960917!

